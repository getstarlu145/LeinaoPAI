动机是ImageNet的网络输入尺寸是224×224，训练集一般缩放到256*256区域后再从中随机裁取224*224作为网络输入；
验证集是将图像缩放到256*256再从中央裁取224*224区域作为网络输入。因此，对于ImageNet中绝大多数500*300大小的图像，
真正参与网络计算的只是其中256*256的区域，而额外的像素也被加载进内存甚至是显存（然而并不参与后续的计算）。在加载过程中，GPU是在等待输入的，
这是是对GPU资源的极大浪费，从用户角度来说使得训练模型的时间明显增加。考虑到batch_size，这个问题将更加严重。

原始的ImageNet数据集将近150GB，将图像缩放到256*256后，新的数据集为73GB。这说明对数据I/O负担减轻了近一半。
建议同时保留原始的和处理后的两个图像数据集，可能有用户需要对输入进行特殊的数据增强，但裁剪后的数据集对绝大多数用户来说都是更合适的。

文件说明：
convert_img.sh：将ImageNet文件夹下的所有图像按照短边缩放到256（如果图像尺寸小于256则不处理）。
dataset_imagenetfolder.py：对处理后的ImageNet数据集进行加载，返回DataLoader实例，直接用于网络训练和测试。

PyTorch ImageFolder读取图片是单张的，但它底层做好了zero-cost cross-thread communication，
能避开python GIL的限制充分利用多核。